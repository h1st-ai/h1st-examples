{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import h1st.core as h1\n",
    "h1.init()\n",
    "\n",
    "from examples.RayTune.sklearn_smv_classifier import SklearnSVMClassifier\n",
    "from examples.RayTune.tensorflow_mlp_classifier import TensorflowMLPClassifier\n",
    "from examples.RayTune import config\n",
    "from examples.RayTune.utils import prepare_train_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune's Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.grid_search([0.005]),\n",
    "    \"units\": tune.grid_search([16, 32, 64, 128]),\n",
    "    \"n_layer\": tune.grid_search([3, 5, 10])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = './data/credit_card'\n",
    "\n",
    "def train_model_with_tune(config):\n",
    "    from ray.tune import report\n",
    "    import os\n",
    "    from filelock import FileLock    \n",
    "    h1_tf_mlp = TensorflowMLPClassifier(\n",
    "        units=config['units'], lr=config['lr'], n_layer=config['n_layer'])\n",
    "    \n",
    "    lock_file = f'{DATA_ROOT}/data.lock'\n",
    "    if not os.path.exists(DATA_ROOT):\n",
    "        os.makedirs(DATA_ROOT)\n",
    "    with FileLock(os.path.expanduser(lock_file)):\n",
    "        data = h1_tf_mlp.load_data()\n",
    "    prepared_data = h1_tf_mlp.prep(data) \n",
    " \n",
    "    h1_tf_mlp.train(prepared_data)\n",
    "    h1_tf_mlp.evaluate(prepared_data)\n",
    "    for _ in range(5):\n",
    "        report(iterations=100, mean_accuracy=h1_tf_mlp.metrics['accuracy'])    \n",
    "\n",
    "#     h1_tf_mlp.persist('my_tf_mlp')\n",
    "#     report(mean_accuracy=h1_tf_mlp.metrics['accuracy'])\n",
    "\n",
    "#     train_loader, test_loader = get_data_loaders()\n",
    "#     model = ConvNet()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config['momentum'])\n",
    "#     for i in range(10):\n",
    "#         train(model, optimizer, train_loader)\n",
    "#         acc = test(model, test_loader)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_func = tune.run(train_model_with_tune, config=config, stop={\"training_iteration\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best config: \", analysis_func.get_best_config(metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_func.dataframe().sort_values('mean_accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_func.dataframe()[['mean_accuracy', 'config/lr', 'config/n_layer', 'config/units']].sort_values('mean_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis_func.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1_tf_mlp = TensorflowMLPClassifier(units=32, lr=0.01)\n",
    "# data = h1_tf_mlp.load_data()\n",
    "# prepared_data = h1_tf_mlp.prep(data)\n",
    "# h1_tf_mlp.train(prepared_data)\n",
    "# h1_tf_mlp.evaluate(prepared_data)\n",
    "# h1_tf_mlp.metrics\n",
    "# h1_tf_mlp.persist('my_tf_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune's Class API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainH1stModel(tune.Trainable):\n",
    "    def _setup(self, config):\n",
    "        from ray.tune import report\n",
    "        import os\n",
    "        from filelock import FileLock            \n",
    "        self.hyper_parameters = {\n",
    "            'units': config['units'], 'n_layer': config['n_layer']}\n",
    "        self.h1_tf_mlp = TensorflowMLPClassifier(lr=0.01, config['units'],  config['n_layer'])\n",
    "        DATA_ROOT = './data/credit_card'\n",
    "        lock_file = f'{DATA_ROOT}/data.lock'\n",
    "        if not os.path.exists(DATA_ROOT):\n",
    "            os.makedirs(DATA_ROOT)\n",
    "        with FileLock(os.path.expanduser(lock_file)):\n",
    "            data = self.h1_tf_mlp.load_data()\n",
    "        self.prepared_data = self.h1_tf_mlp.prep(data)         \n",
    "        \n",
    "    def _train(self):\n",
    "        self.h1_tf_mlp.train(self.prepared_data)\n",
    "        self.h1_tf_mlp.evaluate(self.prepared_data)\n",
    "        acc = self.h1_tf_mlp.metrics['accuracy']\n",
    "        return {\"mean_accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": tune.grid_search([0.005]),\n",
    "    \"units\": tune.grid_search([16, 32, 64, 128]),\n",
    "    \"n_layer\": tune.grid_search([3, 5, 10])\n",
    "#     \"n_layer\": tune.uniform(1, 10)\n",
    "}\n",
    " \n",
    "analysis = tune.run(\n",
    "    TrainH1stModel, \n",
    "    config=config, \n",
    "    verbose=2,\n",
    "    stop={\"training_iteration\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Base Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class TrainH1stModel(tune.Trainable):\n",
    "    def _setup(self, config):\n",
    "        from filelock import FileLock \n",
    "        self.config = config\n",
    "        self.timestep = 0\n",
    "        self.h1_tf_mlp = TensorflowMLPClassifier(\n",
    "            units=int(config['units']), \n",
    "            epochs=int(config['epochs']), \n",
    "            lr=config['lr'], \n",
    "            n_layer=int(config['n_layer']))\n",
    "        DATA_ROOT = './data/credit_card'\n",
    "        lock_file = f'{DATA_ROOT}/data.lock'\n",
    "        if not os.path.exists(DATA_ROOT):\n",
    "            os.makedirs(DATA_ROOT)\n",
    "        with FileLock(os.path.expanduser(lock_file)):\n",
    "            data = self.h1_tf_mlp.load_data()\n",
    "        self.prepared_data = self.h1_tf_mlp.prep(data)         \n",
    "        \n",
    "    def _train(self):\n",
    "        self.timestep += 1\n",
    "        self.h1_tf_mlp.train(self.prepared_data)\n",
    "        self.h1_tf_mlp.evaluate(self.prepared_data)\n",
    "\n",
    "        # tf_mlp_{timestep}_{lr}_{layer}_{unit}_{epochs}_{ts}\n",
    "        version_name = \"tf_mlp_\" + (\n",
    "            \"_\".join(f\"{k}:{v}\" for k, v in self.config.items())\n",
    "        ) + \"_\" + str(int(datetime.datetime.utcnow().timestamp()))\n",
    "\n",
    "        version = self.h1_tf_mlp.persist(version_name)\n",
    "        acc = self.h1_tf_mlp.metrics['accuracy']\n",
    "        return {\"mean_accuracy\": acc, 'model_version': version}\n",
    "    \n",
    "    def _save(self, checkpoint_dir):\n",
    "        import json\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(json.dumps({\"timestep\": self.timestep}))\n",
    "        return path\n",
    "\n",
    "    def _restore(self, checkpoint_path):\n",
    "        import json\n",
    "        with open(checkpoint_path) as f:\n",
    "            self.timestep = json.loads(f.read())[\"timestep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX 2\n",
    "\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr='training_iteration',\n",
    "        perturbation_interval=2,  # Every N time_attr units, \"perturb\" the parameters.\n",
    "        hyperparam_mutations={           \n",
    "            \"lr\": [0.005],\n",
    "            \"epochs\": [2, 10, 30, 100],\n",
    "            \"units\": [4, 8, 16, 32, 64],\n",
    "            \"n_layer\": [2, 4, 6]            \n",
    "        },\n",
    "        **experiment_metrics)\n",
    "\n",
    "# Note: This appears to be needed to avoid a \"key error\", but in fact these values won't change\n",
    "# in the analysis.dataframe() object, even though they will be tuned by the PBT scheduler.\n",
    "# So when you look at the analysis.dataframe(), look at the `experiment_tag` to see the actual values!\n",
    "config = {\n",
    "    \"lr\": 0.005,            # Use the lowest values from the previous definition\n",
    "    \"units\": 16,\n",
    "    \"n_layer\": 4,\n",
    "    \"epochs\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XXX 3\n",
    "\n",
    "analysis = tune.run(TrainH1stModel, \n",
    "    scheduler=pbt_scheduler, \n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 4},\n",
    "#     stop={\"mean_accuracy\": 0.82, \"training_iteration\": 200},                    \n",
    "    num_samples=40,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[['trial_id', 'mean_accuracy', 'config/lr', 'config/n_layer', 'config/units']].sort_values('mean_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO Hyperband (BOHB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import ray\n",
    "# from ray import tune\n",
    "# from ray.tune import Trainable\n",
    "# from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "# from ray.tune.suggest.bohb import TuneBOHB\n",
    "\n",
    "\n",
    "# class MyTrainableClass(Trainable):\n",
    "#     \"\"\"Example agent whose learning curve is a random sigmoid.\n",
    "\n",
    "#     The dummy hyperparameters \"width\" and \"height\" determine the slope and\n",
    "#     maximum reward value reached.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def setup(self, config):\n",
    "#         self.timestep = 0\n",
    "\n",
    "#     def step(self):\n",
    "#         self.timestep += 1\n",
    "#         v = np.tanh(float(self.timestep) / self.config.get(\"width\", 1))\n",
    "#         v *= self.config.get(\"height\", 1)\n",
    "\n",
    "#         # Here we use `episode_reward_mean`, but you can also report other\n",
    "#         # objectives such as loss or accuracy.\n",
    "#         return {\"episode_reward_mean\": v}\n",
    "\n",
    "#     def save_checkpoint(self, checkpoint_dir):\n",
    "#         path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "#         with open(path, \"w\") as f:\n",
    "#             f.write(json.dumps({\"timestep\": self.timestep}))\n",
    "#         return path\n",
    "\n",
    "#     def load_checkpoint(self, checkpoint_path):\n",
    "#         with open(checkpoint_path) as f:\n",
    "#             self.timestep = json.loads(f.read())[\"timestep\"]\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import ConfigSpace as CS  # noqa: F401\n",
    "#     ray.init(num_cpus=8)\n",
    "\n",
    "#     config = {\n",
    "#         \"iterations\": 100,\n",
    "#         \"width\": tune.uniform(0, 20),\n",
    "#         \"height\": tune.uniform(-100, 100),\n",
    "#         \"activation\": tune.choice([\"relu\", \"tanh\"])\n",
    "#     }\n",
    "\n",
    "#     # Optional: Pass the parameter space yourself\n",
    "#     # config_space = CS.ConfigurationSpace()\n",
    "#     # config_space.add_hyperparameter(\n",
    "#     #     CS.UniformFloatHyperparameter(\"width\", lower=0, upper=20))\n",
    "#     # config_space.add_hyperparameter(\n",
    "#     #     CS.UniformFloatHyperparameter(\"height\", lower=-100, upper=100))\n",
    "#     # config_space.add_hyperparameter(\n",
    "#     #     CS.CategoricalHyperparameter(\n",
    "#     #         \"activation\", choices=[\"relu\", \"tanh\"]))\n",
    "\n",
    "#     experiment_metrics = dict(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "#     bohb_hyperband = HyperBandForBOHB(\n",
    "#         time_attr=\"training_iteration\",\n",
    "#         max_t=100,\n",
    "#         reduction_factor=4,\n",
    "#         **experiment_metrics)\n",
    "\n",
    "#     bohb_search = TuneBOHB(\n",
    "#         # space=config_space,  # If you want to set the space manually\n",
    "#         max_concurrent=4,\n",
    "#         **experiment_metrics)\n",
    "\n",
    "#     tune.run(\n",
    "#         MyTrainableClass,\n",
    "#         name=\"bohb_test\",\n",
    "#         config=config,\n",
    "#         scheduler=bohb_hyperband,\n",
    "#         search_alg=bohb_search,\n",
    "#         num_samples=10,\n",
    "#         stop={\"training_iteration\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigSpace as CS\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = CS.ConfigurationSpace()\n",
    "\n",
    "# There are also UniformIntegerHyperparameter and UniformFloatHyperparameter\n",
    "# objects for defining integer and float ranges, respectively. For example:\n",
    "# config_space.add_hyperparameter(\n",
    "#     CS.UniformIntegerHyperparameter('foo', lower=0, upper=100))\n",
    "\n",
    "# config_space.add_hyperparameter(\n",
    "#     CS.UniformFloatHyperparameter('lr', lower=0.001, upper=0.1))\n",
    "# config_space.add_hyperparameter(\n",
    "#     CS.UniformIntegerHyperparameter('units', lower=1, upper=64))\n",
    "# config_space.add_hyperparameter(\n",
    "#     CS.UniformIntegerHyperparameter('n_layer', lower=1, upper=8))\n",
    "\n",
    "config_space.add_hyperparameter(\n",
    "    CS.CategoricalHyperparameter('lr', choices=[0.005]))\n",
    "config_space.add_hyperparameter(\n",
    "    CS.CategoricalHyperparameter('units', choices=[4, 8, 16, 32]))\n",
    "config_space.add_hyperparameter(\n",
    "    CS.CategoricalHyperparameter('n_layer', choices=[2, 4, 6, 8]))\n",
    "config_space.add_hyperparameter(\n",
    "    CS.CategoricalHyperparameter('epochs', choices=[2, 10, 30, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics = dict(metric=\"mean_accuracy\", mode=\"max\")\n",
    "search_algorithm = TuneBOHB(config_space, max_concurrent=4, **experiment_metrics)\n",
    "# search_algorithm = TuneBOHB(config_space, max_concurrent=4)\n",
    "\n",
    "scheduler = HyperBandForBOHB(\n",
    "    time_attr='training_iteration',\n",
    "    reduction_factor=4,)\n",
    "#     **experiment_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    TrainH1stModel, \n",
    "    scheduler=scheduler, \n",
    "    search_alg=search_algorithm, \n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",         \n",
    "    num_samples=40,                           # Force it try all 12 combinations\n",
    "    stop={\"training_iteration\": 4},\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[['mean_accuracy', 'config/epochs','config/lr', 'config/n_layer', 'config/units']].sort_values('mean_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization (BO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This test checks that BayesOpt is functional.\n",
    "\n",
    "It also checks that it is usable with a separate scheduler.\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "\n",
    "# def evaluation_fn(step, width, height):\n",
    "#     return (0.1 + width * step / 100)**(-1) + height * 0.1\n",
    "\n",
    "\n",
    "# def easy_objective(config):\n",
    "#     # Hyperparameters\n",
    "#     width, height = config[\"width\"], config[\"height\"]\n",
    "\n",
    "#     for step in range(config[\"steps\"]):\n",
    "#         # Iterative training function - can be any arbitrary training procedure\n",
    "#         intermediate_score = evaluation_fn(step, width, height)\n",
    "#         # Feed the score back back to Tune.\n",
    "#         tune.report(iterations=step, mean_loss=intermediate_score)\n",
    "#         time.sleep(0.1)\n",
    "\n",
    "space = {\n",
    "    \"lr\": tune.uniform(0.005, 0.05),\n",
    "    \"units\": tune.quniform(1, 64, 1),\n",
    "#     \"units\": tune.randint(1, 64),\n",
    "#     \"n_layer\": tune.randint(1, 8)\n",
    "    \"n_layer\": tune.quniform(1, 8, 1),    \n",
    "    \"epochs\": tune.quniform(1, 50, 1),\n",
    "}\n",
    "\n",
    "algo = BayesOptSearch(   \n",
    "    utility_kwargs={\n",
    "        \"kind\": \"ucb\",\n",
    "        \"kappa\": 2.5,\n",
    "        \"xi\": 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "# scheduler = AsyncHyperBandScheduler()\n",
    "analysis= tune.run(\n",
    "    TrainH1stModel,\n",
    "    config=space,\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",     \n",
    "#     name=\"my_exp\",\n",
    "    search_alg=algo,\n",
    "#     scheduler=scheduler,\n",
    "    stop={\"training_iteration\": 4},\n",
    "    verbose=1,\n",
    "    num_samples=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()[['mean_accuracy', 'config/lr', 'config/n_layer', 'config/units']].sort_values('mean_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
